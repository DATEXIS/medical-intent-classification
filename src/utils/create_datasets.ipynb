{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from Levenshtein import distance\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_surgery(data):\n",
    "    tkn = \"Surgery\"\n",
    "    if tkn in data.intents:\n",
    "        data.intents.remove(tkn)\n",
    "        if \"Other Treatments\" not in data.intents:\n",
    "            data.intents.append(\"Other Treatments\")\n",
    "    return data\n",
    "\n",
    "def prepare_next_intent_prediction_data(sample, num_prev, source, intents_only):\n",
    "    all_turns = source[source[\"encounter_id\"] == sample.encounter_id]\n",
    "    text = \"\"\n",
    "    #print(sample.name)\n",
    "    if num_prev != 0:\n",
    "        for i in reversed(range(num_prev)):\n",
    "            if (sample.text_id-(i+1)) >= 0:\n",
    "                prev_turn = all_turns[all_turns[\"text_id\"] == (sample.text_id-(i+1))]\n",
    "                if not prev_turn.empty:\n",
    "                    prev_turn_text = prev_turn.text.item().replace(\"doctor:\", \"[Doctor]\").replace(\"patient:\", \"[Patient]\").replace(\"patient_guest:\", \"[Patient Guest]\")\n",
    "                    for intent in prev_turn.intents.item():\n",
    "                        text += \"[\" + intent + \"]\"\n",
    "                    if not intents_only:\n",
    "                        text += prev_turn_text + \"[SEP]\"\n",
    "                    else:\n",
    "                        text += \"[SEP]\"\n",
    "            else:\n",
    "                \n",
    "                if \"[Conversation Start]\" not in text:\n",
    "                    text += \"[Conversation Start][SEP]\"\n",
    "    else:\n",
    " \n",
    "        for i in range(sample.text_id):\n",
    "            if i == 0:\n",
    "                text += \"[Conversation Start]\"\n",
    "            prev_turn = all_turns[all_turns[\"text_id\"] == i]\n",
    "            if not prev_turn.empty:\n",
    "                prev_turn_text = prev_turn.text.item().replace(\"doctor:\", \"[Doctor]\").replace(\"patient:\", \"[Patient]\").replace(\"patient_guest:\", \"[Patient Guest]\")\n",
    "                for intent in prev_turn.intents.item():\n",
    "                    text += \"[\" + intent + \"]\"\n",
    "                if not intents_only:\n",
    "                    text += prev_turn_text + \"[SEP]\"\n",
    "\n",
    "    return text if text != \"\" else None\n",
    "def map_speciality(sample, specialities_by_id):\n",
    "    for speciality in specialities_by_id.iterrows():\n",
    "        if sample.encounter_id in literal_eval(speciality[1][1]):\n",
    "            spec = speciality[1][0]\n",
    "    return spec\n",
    "\n",
    "def map_sections(sample):\n",
    "    subj = [\"Acute Symptoms\", \"Personal History\", \"Drug History\", \"Vegetative History\", \"Other Socials\", \"Family History\", \"Greetings\", \"Therapeutic History\"]\n",
    "    obj = [\"Physical Examination\", \"Lab Examination\", \"Radiology Examination\"]\n",
    "    ass =[\"Acute Assessment\", \"Reassessment\"]\n",
    "    plan = [\"Discussion\", \"Referral\", \"Medication\", \"Follow-up\", \"Other Treatments\", \"Diagnostic Testing\"]  \n",
    "    null = [\"Chitchat\"]\n",
    "    sections = []\n",
    "    for intent in sample.intents:\n",
    "        if intent in subj:\n",
    "            sections.append(\"Subjective\")\n",
    "        elif intent in obj:\n",
    "            sections.append(\"Objective\")\n",
    "        elif intent in plan:\n",
    "            sections.append(\"Plan\")\n",
    "        elif intent in ass:\n",
    "            sections.append(\"Assessment\")\n",
    "        elif intent in null:\n",
    "            sections.append(\"Null\")\n",
    "        else:\n",
    "            print(f\"Error: {sample.name}\")\n",
    "    return list(set(sections))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json(\"\")\n",
    "train[\"sections\"] = train.apply(map_sections, axis=1)\n",
    "val = pd.read_json(\"\")\n",
    "val[\"sections\"] = val.apply(map_sections, axis=1)\n",
    "test = pd.read_json(\"\")\n",
    "test[\"sections\"] = test.apply(map_sections, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(\"\")\n",
    "data.intents = data.intents.map(lambda x: [x] if type(x) == str else x)\n",
    "data.sections = data.sections.map(lambda x: [x] if type(x) == str else x)\n",
    "data = data.apply(move_surgery, axis=1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_intents_lookup = {}\n",
    "unique_intents_lookup_id = 0\n",
    "for row_id, row in data.iterrows():\n",
    "    intent_string = \",\".join(row.intents)\n",
    "    if intent_string not in unique_intents_lookup.keys():\n",
    "        unique_intents_lookup[intent_string] = unique_intents_lookup_id\n",
    "        unique_intents_lookup_id += 1\n",
    "unique_sections_lookup = {}\n",
    "unique_sections_lookup_id = 0\n",
    "for row_id, row in data.iterrows():\n",
    "    section_string = \",\".join(row.sections)\n",
    "    if section_string not in unique_sections_lookup.keys():\n",
    "        unique_sections_lookup[section_string] = unique_sections_lookup_id\n",
    "        unique_sections_lookup_id += 1      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gather_intent_sequence(conv, lookup=unique_intents_lookup):\n",
    "    sequence_list = []\n",
    "    for intents in conv[\"intents\"]:\n",
    "        sequence_list.append(lookup[\",\".join(intents)])\n",
    "    return sequence_list\n",
    "def gather_section_sequence(conv, lookup=unique_sections_lookup):\n",
    "    sequence_list = []\n",
    "    for sections in conv[\"sections\"]:\n",
    "        sequence_list.append(lookup[\",\".join(sections)])\n",
    "    return sequence_list\n",
    "def calculate_sequence_similarity(sequence, encounter_id, sequence_id, all_sequences, special_index=None):\n",
    "    all_sequences = all_sequences.drop(sequence_id)\n",
    "\n",
    "    tmp_mask = special_index[\"0\"].map(lambda x:True if encounter_id in x else False)\n",
    "    speciality_index = literal_eval(special_index[\"0\"][tmp_mask].item())\n",
    "    speciality_index.remove(encounter_id)\n",
    "    filtered_sequences = all_sequences.query(\"encounter_id in @speciality_index\")\n",
    "    filtered_distances = filtered_sequences[\"section_sequences\"].map(lambda x: distance(sequence, x))\n",
    "    \n",
    "\n",
    "    all_distances = all_sequences[\"section_sequences\"].map(lambda x: distance(sequence, x))\n",
    "    closest_id = all_sequences[\"encounter_id\"].loc[all_distances.idxmin()]\n",
    "    return pd.Series({\"avg_all_dist\":all_distances.mean(), \"all_all_dist\": all_distances.to_dict(), \"avg_filtered_dist\":filtered_distances.mean(), \"filtered_all_dist\": filtered_distances.to_dict(),\"closest_id\":closest_id, \"share_speciality\":closest_id in speciality_index})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = data.groupby(\"encounter_id\").apply(lambda x: x.drop(\"encounter_id\", axis=1).to_dict(orient=\"list\")).reset_index(name=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped[\"section_sequences\"] = grouped.data.map(gather_section_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"next_i_pred_src\"] = data.apply(lambda x: prepare_next_intent_prediction_data(x, 5, data, False), axis=1)\n",
    "n_i_p_data = data[[\"next_i_pred_src\", \"intents\", \"id\"]]\n",
    "n_i_p_data.dropna(inplace=True)\n",
    "n_i_p_data = n_i_p_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.text = data.text.map(lambda x: x.replace(\"doctor:\", \"[Doctor]\").replace(\"patient:\", \"[Patient]\").replace(\"patient_guest:\", \"[Patient Guest]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stratify next intent prediction\n",
    "n_i_p_data[\"tuples\"] = n_i_p_data.intents.apply(tuple)\n",
    "unique_rows =n_i_p_data[~n_i_p_data[\"tuples\"].duplicated(keep=False)]\n",
    "n_i_p_data = n_i_p_data.drop(unique_rows.index)\n",
    "# Example n_i_p_dataset\n",
    "n_i_p_data = n_i_p_data[[\"next_i_pred_src\", \"intents\"]]\n",
    "\n",
    "# Split into train+dev and test with stratification\n",
    "train_dev, test = train_test_split(n_i_p_data, test_size=0.15, stratify=n_i_p_data[\"intents\"], random_state=42)\n",
    "\n",
    "# Split train+dev into train and dev with stratification\n",
    "train, dev = train_test_split(train_dev, test_size=0.15, stratify=train_dev['intents'], random_state=42)  # 0.25 * 0.8 = 0.2\n",
    "\n",
    "# Print the sizes of each split\n",
    "print(f\"Train size: {len(train)}, Dev size: {len(dev)}, Test size: {len(test)}\")\n",
    "\n",
    "# Verify the stratification\n",
    "print(\"Train label distribution:\\n\", train['intents'].value_counts(normalize=True))\n",
    "print(\"Dev label distribution:\\n\", dev['intents'].value_counts(normalize=True))\n",
    "print(\"Test label distribution:\\n\", test['intents'].value_counts(normalize=True))\n",
    "train = pd.concat((train, unique_rows[[\"next_i_pred_src\", \"intents\"]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stratify intent prediction\n",
    "data[\"tuples\"] = data.intents.apply(tuple)\n",
    "unique_rows = data[~data[\"tuples\"].duplicated(keep=False)]\n",
    "data = data.drop(unique_rows.index)\n",
    "# Example dataset\n",
    "data = data[[\"text\", \"intents\", \"sections\"]]\n",
    "\n",
    "# Split into train+dev and test with stratification\n",
    "train_dev, test = train_test_split(data, test_size=0.15, stratify=data[\"intents\"], random_state=42)\n",
    "\n",
    "# Split train+dev into train and dev with stratification\n",
    "train, dev = train_test_split(train_dev, test_size=0.15, stratify=train_dev['intents'], random_state=42)  # 0.25 * 0.8 = 0.2\n",
    "\n",
    "# Print the sizes of each split\n",
    "print(f\"Train size: {len(train)}, Dev size: {len(dev)}, Test size: {len(test)}\")\n",
    "\n",
    "# Verify the stratification\n",
    "print(\"Train label distribution:\\n\", train['intents'].value_counts(normalize=True))\n",
    "print(\"Dev label distribution:\\n\", dev['intents'].value_counts(normalize=True))\n",
    "print(\"Test label distribution:\\n\", test['intents'].value_counts(normalize=True))\n",
    "train = pd.concat((train, unique_rows[[\"text\", \"intents\", \"sections\"]]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
